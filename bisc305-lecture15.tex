
\lecture[15]{More on hypothesis testing}{lecture-text}

\subtitle{checking conditions}

\date{24 October 2013}


\begin{document}

\begin{frame}
  \maketitle
\end{frame}


\begin{frame}{Last time}
  What is, and isn't, implied by ``statistically significant'' (or not).
  \begin{enumerate}
    \item Statistical significance means an observed effect probably wasn't the result of chance in sampling,
    \item   \ldots and lack of statistical significance means that it could have been.
    \item The $P$-value says nothing about how big the effect actually is,
    \item   since the sample size ``acts like a magnifying glass''.
    \item The \alert{effect size} is one way to think about how important an effect is.
    \item Confidence intervals can communicate both statistical significance,
      and bounds on the absolute size of an effect.
  \end{enumerate}
\end{frame}

\begin{frame}\frametitle<presentation>{Outline}
  \tableofcontents
\end{frame}


\section{Conditions for (best) use of a $t$-test}

\begin{frame}{What are ``conditions''?}

  Statistical tests usually work by finding the probability that 
  \begin{itemize}
    \item something happens,
    \item under a certain generative model.
  \end{itemize}

  \vspace{2em}

  \structure{Example:} The $P$-value for the $t$-test is the probability that 
  \begin{itemize}
    \item the difference in sample means between indpendent samples from two populations is at least as big as the observed value ($\bar y_1 - \bar y_2$), 
    \item if the two populations have the same mean ($\mu_1 = \mu_2$), and the sampling distribution of the sample mean is Normal.
  \end{itemize}

  \vspace{2em}
  
  \alert{Conditions}, a.k.a.\ ``assumptions'', come from the second point.  
  If they are not true, we might
  \begin{itemize}
    \item have \alert{wrong} $P$-values (i.e.\ misreport the probability of type I error)
    \item \structure{and/or} have lower power than a better test (i.e.\ misestimate the probability of type II error)
  \end{itemize}

\end{frame}

\subsection{The $t$-test}

%%%%%%
\begin{frame}{Conditions for the $t$-test}

  \begin{block}{Conditions}
    \begin{enumerate}
      \item The sampling procedure provides:
        \begin{enumerate}
          \item random, independent samples from large populations,
          \item with the two samples independent of each other.
        \end{enumerate}
      \item The sampling distributions of $\bar Y_1$ and $\bar Y_w$ are
        \begin{enumerate}
          \item close enough to Normal.
      \end{enumerate}
    \end{enumerate}
  \end{block}

  \vspace{2em}

  Sampling distributions of sample means are close to Normal if the sample sizes are large enough.\\

  \vspace{2em}

  ``\alert{Large enough}'' is at least 20, but is larger, 
  the further the \alert{population distributions} are from Normal.

\end{frame}

%%%%%%
\begin{frame}{Simple example}

  \structure{Last time} we saw:

  \vspace{2em}


  Body weight:
  \begin{center}
    \begin{tabular}{crr}
       & Males & Females \\
       \hline
       $n$ & 2 & 2 \\
       $\bar y$ & 175 & 143 \\
       $s$ & 35 & 34
     \end{tabular}

   \vspace{2em}

   {$t_s=0.93$ and $P=0.45$}\\
   CI for $\bar \mu_1 - \bar \mu_2$: $(-117,181)$
   \end{center}

   \vspace{2em}
  
   \alert{Are there any red flags here?}
   
\end{frame}


%%%%%%%%%%%
\subsection{Transforming the data}

%%%%%%
\begin{frame}{Transformations}

    \begin{block}{Transforming the data}
        means applying a function to the data before doing statistics,\\
        for example
        \[  Y = \text{growth rate} \longrightarrow Y = \log( \text{growth rate} ) ). \]
    \end{block}

    \vspace{2em}

    \structure{Goal:} make the sampling distribution of $\bar Y$ closer to Normal,\\
    by making the population distribution of $Y$ closer to Normal.

    \vspace{2em}

    \structure{This is a good idea if}
    \begin{enumerate}
        \item The original data do not look Normally distributed,
        \item the transformed data look closer to Normal,
        \item and the statistics you are using require something to be Normal.
    \end{enumerate}

    \vspace{2em}

    \structure{Remember:} informally, ``Normal'' means ``bell-shaped''.

\end{frame}


\subsection{Simulation: live example}

%%%%%%
\begin{frame}{Real fake data:}
    
    XXX population distributions are exponential (parallel dotplots)

    XXX sampling distribution of mean becomes normal with sample size

    XXX log-transform improves things

\end{frame}



%%%%%% %%%%%%
\section{More about hypothesis testing}

\subsection{How to pick the hypotheses?}

%%%%%%
\begin{frame}{Hypotheses}

    Hypothesis testing requires 
    \begin{itemize}
        \item[$H_0$:] null hypothesis
        \item[$H_A$:] alternative hypothesis
    \end{itemize}

    \vspace{2em}

    \structure{Functional difference:} the null hypothesis is what you evaluate the $P$-value with.


\end{frame}


%%%%%%
\begin{frame}{A dialog}
    \begin{itemizew}{1em}
        \item[You:] It looks like $A$s are better than $B$s.  They tend to have more $X$.
        \item[Skeptic:] But what if they're the same, on average, you just happened to see more $X$ in your sample of $A$s?
        \item[You:] Ok, fine.  Suppose, for the sake of argument, that there \alert{isn't} a difference.
            Then let's see what the chance of seeing this much more $X$ is\ldots
    \end{itemizew}

    \vspace{2em}

    \structure{Note:}
    Often (like in this example), 
        \[ H_0: \mu_1 = \mu_2 , \]
    but not always.

\end{frame}


%%%%%%
\begin{frame}{Example:}

\end{frame}



\subsection{Interpreting $P$-values}


%%%%%%
\begin{frame}{$P$-values}

    \begin{block}{}
        The \alert<1>{$P$-value} of a \alert<2>{result} is the probability of seeing a result \alert<3>{so extreme}, if $H_0$ is true.
    \end{block}

    \vspace{2em}

    \begin{itemizew}{4em}
        \item[``result'':] the test statistic (ex: $\bar y_1 - \bar y_2$)
        \item[``so extreme'':] defined by $H_A$ (ex: $\bar y_1 - \bar y_2 > C$)
        \item[``if $H_0$ is true'':] how to compute the probability.
    \end{itemizew}

\end{frame}


\subsection{Medical testing example}

%%%%%%
\begin{frame}{Example:}

    A medical test for an illness:
    \begin{itemize}
        \item 1\% of population has the illness,
        \item 80\% chance of (true) detection if patient has the illness, and
        \item 5\% chance of (false) detection if patient does not have the illness.
    \end{itemize}

    \vspace{2em}

    With $H_0$: patient has the disease, what is the $\alpha$-level of this test?\\
    If we test 1,000 patients, how many do we expect of:
    \begin{enumerate}
        \item falsely rejected null hypotheses? (type I errors)
        \item falsely accepted null hypotheses? (type II errors)
    \end{enumerate}

    \vspace{2em}

    \structure{True or false:} 
    We have found significant evidence for $H_A$ at the 5\% level,
    so the probability that $H_0$ is true is 5\%.

    \vspace{2em}

    What is a true statement similar to the above?


\end{frame}



% . . . 

\section<article>{Summary}
\section<presentation>*{Summary}

\begin{frame}{Summary}
  \begin{enumerate}
  \item Ein \alert{Wort} ist eine Folge von Symbolen aus einem
    \alert{Alphabet}. 
  \item Eine \alert{Syntax} besteht aus Regeln, nach denen
    Worte (Texte) gebaut werden dürfen.
  \item Eine \alert{Semantik} legt fest, was Worte \alert{bedeuten}.
  \item Eine \alert{formale Sprache} ist eine Menge von Worten
    über einem Alphabet.
  \end{enumerate}
\end{frame}

% homework
\begin{frame}{Homework}
  \begin{center}

  % 7.4.2

  \vspace{2em}

  % 7.4.8

  \vspace{2em}

  % 7.5.1

  \end{center}
\end{frame}


\end{document}





