% Copyright 2007 by Till Tantau

%
% This file may be distributed and/or modified
%
% 1. under the LaTeX Project Public License and/or
% 2. under the GNU Public License.
%
% See the file doc/licenses/LICENSE for more details.


\lecture[12]{Power and false positives}{lecture-text}

\subtitle{hypothesis testing, and one-sided tests}

\date{5 March 2015}

% pp. 218-(242?)


\begin{document}

\begin{frame}
  \maketitle
\end{frame}


\begin{frame}\frametitle<presentation>{Outline}
  \tableofcontents
\end{frame}

%%%%%
\begin{frame}{Last time}

  \begin{itemize}
    \item \structure{Null hypothesis, $H_0$:} ``what if this was just random noise?''
    \item \structure{$P$-value:} the probability of seeing something as suprising as we did, if that was true.
    \item the $t$-test: 
      \begin{itemize}
        \item ``The sample means differ, but could the population means be the same?''
        \item $t_s = (\bar y_1 - \bar y_2) / \SE_{\bar Y_1 - \bar Y_2}$
        \item also need: degrees of freedom.
      \end{itemize}
    \item \structure{Significance value:} $\alpha$, the cutoff for ``good evidence against $H_0$.''
  \end{itemize}

\end{frame}


%%%%% %%%%%
\section{Confidence intervals and $P$-values}


%%%%%
\begin{frame}{Confidence intervals and $P$-values}

    Recall that
  for the $t$-test,
  \[  \text{reject $H_0$ at the $\alpha$ significance level} \]
  is \structure{equivalent to}
  \[  \text{ 0 is not in the $(1-\alpha)$-confidence interval for $\mu_1-\mu_2$ }. \]

  \vspace{2em}

  This is because of \alert{symmetry}: \\
  the distribution of the $t$-statistic always \structure{looks the same},
  no matter the true value of $\mu_1 - \mu_2$,
  just \structure{shifted}.

\end{frame}

%%%%%
\begin{frame}{Example: }

    Noreprenephrin (NE) concentration (ng/gm) in rat brains:
    \begin{center}
      \begin{tabular}{c|rr}
            & Toluene & Control \\
          \hline
          $n$ & 6 & 5 \\
          $\bar y$ & 541.8 & 444.2 \\
          $s$  & 66.1 & 69.6 \\
          $\SE$ & 27 & 31 \\
     \end{tabular}
   \end{center}


     \begin{align*}
         \SE_{\bar Y_1 - \bar Y_2} &= 41.195 \\
         t_s &= 2.34  \\
         \df &= 8.47 
     \end{align*}
     $P$-value: 0.0454 .


  \vspace{2em}

  Here $t_{0.025} = 2.28$, so an $1-\alpha=0.95$ confidence interval is
  \[  (541.8-444.2) \pm 2.28 \times 41.195 = (3.68,191.52) . \]

\end{frame}

%%%%%
\begin{frame}{$P$-value or CI?}

  Which is ``better''?

  \pause
  \begin{itemize}
      \item They use the same information.
      \item Not all tests have a confidence interval.
      \item But, when we do, it tells us the \emph{range} of reasonable values.
      \item \emph{strength of evidence} \structure{versus} \emph{size of effect}
  \end{itemize}

\end{frame}



\section{Hypothesis testing}

%%%%%
\begin{frame}{Interpretation of $\alpha$}

  What the heck is (the significance level) $\alpha$ again?

  \vspace{2em}
  \pause

  It is the 
  \begin{block}{False positive rate:}
    If we did the experiment many times,\\
    \structure{and there was no real effect} ($H_0$ is true)\\
    how often would we \alert{wrongly} conclude there is an effect?
  \end{block}

\end{frame}



%%%%%
\begin{frame}{Example: Music and Marigolds}

    Suppose many researchers run experiments\\
    to test whether marigolds grow taller \\
    when listening to Bach (mean height $\mu_1$) \\
    or Mozart (mean height $\mu_2$). 


    \vspace{2em}

    \begin{align*}
        H_0 &: \quad \mu_1 = \mu_2  \\
        H_A &: \quad \mu_1 \neq \mu_2 
    \end{align*}

    \vspace{2em}

    If everyone uses $\alpha=0.05$ then
    \begin{itemize}
        \item 95\% would find no significant effect
        \item 2.5\% would find statistically significant evidence that Mozart is better
        \item 2.5\% would find statistically significant evidence that Bach is better
    \end{itemize}
    \ldots leading to \alert{controversy!}

\end{frame}


%%%%%
\begin{frame}{Are 90\% of all studies false?}

        (No.)

    \vspace{2em}
    \pause

    \begin{quote}
        \small
        Suppose you throw a dart at the Big Chart O' Human Metabolic Pathways and supplement your experimental group with the chemical you hit. Then ten years later you come back and see how many of them died of heart attacks.
        Most chemicals on the Big Chart probably don't prevent heart attacks. Let's say only one in a thousand do. 

    \vspace{1em}
        
        \pause Maybe your study will successfully find that 1/1000. But the 999 inactive chemicals will also throw up about 50 (999 $\times$ 5\%) false positives significant at the 5\% level. Therefore, even if you conduct your study perfectly, and it shows a significant decrease in heart attacks, there's about a 98\% chance it's false.
    \end{quote}
    \figcaption{http://slatestarcodex.com/2013/02/17/90-of-all-claims-about-the-problems-with-medical-studies-are-wrong/}


\end{frame}

%%%%%
\begin{frame}{Interpreting a significant result:}

    \begin{itemize}
        \item What questions are being asked?
        \item What is the size of the effect?
        \item Are the assumptions met?
        \item How strong is the support?  \\
          (What is the $P$-value?)
    \end{itemize}

\end{frame}

%%%%% %%%%%
\section{Type I and Type II Errors}


%%%%%
\begin{frame}{Significance level}

    The \structure{significance level} $\alpha$ is chosen so
    \[
        \prob \{ \text{significant evidence for $H_A$} | \text{$H_0$ is true} \} = \alpha .
    \]
    This is the probability of a \alert{Type I error}.


    \vspace{2em}

    \begin{tabular}{r|cc}
        & \multicolumn{2}{c}{truth} \\
        & $H_0$ & $H_A$ \\
        \hline
        no evidence for $H_A$ & correct & Type II error \\
        good evidence for $H_A$ & Type I error & correct \\
    \end{tabular}

    \vspace{2em}

    \begin{description}
        \item[Type I:] Our significant result is actually due to random noise.
        \item[Type II:] There's something really happening, but we couldn't distinguish it from random noise.
    \end{description}

\end{frame}

%%%%%
\begin{frame}{History and etymology}

  (from: \textit{``The testing of statistical hypotheses in relation to probabilities a priori".} Jerzy Neyman \& Egon Pearson, 1933. )

    \vspace{2em}

  $H_0$ is the ``alternative to be tested''.
  \begin{quote}
    As a result of the statistical analysis we shall decide either
    \begin{itemize}
        \item[(a)] to accept [the hypothesis to be tested,] $H_0$, 
        \item[(b)] to reject $H_0$, 
        \item[(c)] to remain in doubt 
          on the grounds that the evidence provided by the data is inadequate.
    \end{itemize}
    % A statistical test is therefore equivalent to a rule of the following type,
    % \begin{itemize}
    %   \item[(a)] Reject $H_0$ if $\Sigma$ falls into a region $w$.
    %   \item[(b)] Accept $H_0$ if $\Sigma$ falls into another region $w'$.
    %   \item[(c)] Remain in doubt if $\Sigma$ falls into third region $w''$.
    % \end{itemize}
    In making decisions \textit{(a)} or \textit{(b)}
    we shall sometimes be in error
    ... [and] these errors will be of two kinds:
    \begin{itemize}
      \item[(I)] we reject $H_0$ [i.e., the hypothesis to be tested] when it is true,
      \item[(II)] we accept $H_0$ when some alternative $H_i$ is true.
    \end{itemize}
    The problem before us is to consider how these errors may be best controlled.
  \end{quote}


\end{frame}

%%%%%
\begin{frame}{Power and Type II error}

    \begin{block}{}
        \alert{Statistical power} is the probability of \emph{not} making a Type II error when $H_A$ is true,
        i.e.\ \structure{of detecting a true signal}.
    \end{block}
    

    \vspace{2em}

    It depends on the \structure{strength} of the signal,\\
    the sample size, and the experimental setup.


    \vspace{2em}

    It does \alert{not} depend on (the random model of) $H_0$.\\
    (but it does depend on the test used)

\end{frame}


%%%%%
\begin{frame}{Type I and II and tradeoffs}

    A hierarchy of medical studies:
    \begin{itemize}
        \item Case reports or high-volume scans
        \item Exploratory scans or surveys
        \item Case-control studies
        \item Randomized controlled trials
    \end{itemize}


    \vspace{2em}

    Consider risks and benefits.


\end{frame}



%%%%
\section{One--sided $t$-tests}

\subsection{One versus two sides}


%%%%%
\begin{frame}{Back to the $t$ test}
    
    Maybe we had a \structure{strong directional hypothesis}:\\
    we are throwing away half our power!

    \vspace{2em}

    \structure{Example:}
    Measure infection loads in subjects with ($\bar y_1$) and without ($\bar y_2$) a new antibiotic. 

    \vspace{1em}

    \structure{$H_0$:} antibiotic doesn't affect infection load, $\mu_1 = \mu_2$ \\
    \structure{$H_A$:} antibiotic decreases infection load, $\mu_1< \mu_2$ .

    \vspace{2em}
    \pause

    \alert{What if} infection load is \emph{larger} in samples with antibiotics? ($\bar y_1 > \bar y_2$)


\end{frame}

%
\begin{frame}{One or two sides?}

  \begin{description}

    \item[two-sided test:] ``Could the two groups look \alert{this different} if they were actually the same?''

    \item[one-sided test:] ``Could this group look \alert{this much larger} than the other group if they were actually the same?''

  \end{description}

  \vspace{2em}

  \only<1>{
  \structure{More precisely,}
  \begin{description}

    \item[two-sided test:] ``What's the chance that the difference in sample means was at least this large, if the control and treatment populations actually have the same mean?''

    \item[one-sided test:] ``What's the chance that the treatment sample mean is at least this much larger than the control group sample mean,
      if the control and treatment populations actually have the same mean?''

  \end{description}
  }

  \only<2>{
  \structure{More concisely,} with $H_0: \; \mu_1 = \mu_2$:
  \begin{description}

    \item[two-sided test:] $H_A: \; \mu_1 \neq \mu_2$

    \item[one-sided test:] $H_A: \; \mu_1 > \mu_2$

  \end{description}

  \vspace{2em}

  \structure{Note:} can do the test in either direction, replacing "larger" with "smaller".
  }

\end{frame}

%%%
\subsection{Finding a one-sided $P$-value}

\begin{frame}{How to do a one-sided test}

  \begin{center}
  \includegraphicscopyright[width=.8\textwidth]{one-tailed-tests}{Samuels, Whitmer, \& Schaffner}
  \end{center}

  \vspace{-1em}

  \begin{enumerate}
    \item Check if the difference in means is in the right direction. (if not, $P>0.5$).

    \item Compute the $t$ statistic:
      \[
          t_s = \frac{ (\bar y_1 - \bar y_2) - 0}{ \SE_{(\bar Y_1 - \bar Y_2)} }
      \]

    \item Compute the degrees of freedom:
        \[
            df = \frac{ (\SE_1^2 + \SE_2^2)^2 }{ \SE_1^4 / (n_1-1) + \SE_2^4 / (n_2 - 1) }
        \]

    \item Look up the $P$-value. (or, check for significance in Table 4)

  \end{enumerate}


\end{frame}

%%%%%
\begin{frame}{Example: Niacin supplements}

  One group of lambs fed niacin supplements; one control group.  
  \begin{align*}
    H_0 &: \quad \mu_\text{niacin} = \mu_\text{control}  \\
    H_A &: \quad \mu_\text{niacin} > \mu_\text{control}  
  \end{align*}


    \vspace{2em}

  
  Weight gains (lbs):
    \begin{center}
      \begin{tabular}{c|rr}
            & Niacin & Control \\
          \hline
          $\bar y$ & 14 & 10 \\
          $\SE$ & 1.5 & 1.6 \\
     \end{tabular}
   \end{center}
   and $df = 18$.


    \vspace{2em}
    \pause

    \[
    t_s = \frac{ \bar y_\text{niacin} - \bar y_\text{control} }{ \SE_{\bar Y_1 - \bar Y_2} } = \frac{ 14 - 10 }{ \sqrt{ 1.5^2 + 1.6^2 } } = 1.82
    \]
    and $P = 0.043$.
    \pause
    \structure{What would the $P$-value be for a two-sided test?}

\end{frame}


%%
\subsection{What to watch out for}

%
\begin{frame}{How to cheat}

  \begin{enumerate}
    \item Check which mean is larger.
    \item Do a one-sided test, in that direction.
  \end{enumerate}

  \vspace{2em}

  \structure{Solution:} Have a clear hypothesis, that determines the direction beforehand.

  \vspace{2em}

  \begin{block}{Music \& Marigolds}
    If 100 studies look for an effect that isn't real, 
    how many will ``find'' the effect, with
    \begin{itemize}
      \item[\bf (a)] two-sided tests
      \item[\bf (b)] one-sided tests, and a clear prior hypothesis
      \item[\bf (c)] one-sided tests, cheating
    \end{itemize}
  \end{block}

\end{frame}



\section<article>{Summary}
\section<presentation>*{Summary}

\begin{frame}{Summary}
  \begin{enumerate}
    \item $P$-values or confidence intervals: strength of evidence versus effect size
    \item ``Significance level'' can be interpreted as a \alert{false positive rate}
    \item but somehow science moves forward anyhow.
    \item \structure{Type I} error: wrongly interpreting noise as pattern.
    \item \structure{Type II} error: failing to find pattern amidst noise.
    \item \alert{Statistical power} is the chance of finding a true pattern, \\
      or $1-{}$ type II error probability
    \item If the hypothesis is strongly one-sided,
    \item use a one-sided test, which increases power.
    \item But don't cheat.
  \end{enumerate}
\end{frame}

% homework
\begin{frame}{Homework}
  \begin{center}

      7.3.4

    \vspace{2em}

    7.3.5

    \vspace{2em}

    7.5.5

  \end{center}
\end{frame}


\end{document}





